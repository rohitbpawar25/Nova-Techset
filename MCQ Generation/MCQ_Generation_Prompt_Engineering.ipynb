{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLC-uLt0x7nU",
        "outputId": "01aa1aea-84a7-452c-dc86-5b9ccf0e7695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "GENERATING 10 MCQs → Python (3-5 years) – General Backend / Data Engineering\n",
            "================================================================================\n",
            "**Question 1**  \n",
            "You receive a `MemoryError` when using `pandas.read_csv('huge.tsv', sep='\\t')` on a 50 GB file on a 32 GB RAM machine. Which single-line change keeps the code pure-pandas and most memory-efficient while still allowing grouped aggregations later?\n",
            "\n",
            "A. `df = pd.read_csv('huge.tsv', sep='\\t', dtype={'id': 'int32', 'value': 'float32'})`  \n",
            "B. `df = pd.read_csv('huge.tsv', sep='\\t', chunksize=1_000_000)`  \n",
            "C. `df = pd.read_csv('huge.tsv', sep='\\t', usecols=['id','value'])`  \n",
            "D. `df = pd.read_csv('huge.tsv', sep='\\t', low_memory=False)`  \n",
            "\n",
            "Correct Answer: A  \n",
            "Explanation: Downsizing dtypes converts the file to ~¼ RAM footprint without sacrificing in-memory operations, letting you keep the full dataset for later groupby. Chunksize (B) streams but forces out-of-core work, while usecols (C) only helps if you can drop columns; low_memory (D) is a legacy flag that never reduces footprint.\n",
            "\n",
            "---\n",
            "\n",
            "**Question 2**  \n",
            "A Flask endpoint must return a JSON response containing a `Decimal('3.50')` field. What is the smallest, production-grade change that prevents `TypeError: Object of type Decimal is not JSON serializable`?\n",
            "\n",
            "A. Subclass `flask.json.JSONEncoder` and override `default()` to handle `Decimal`.  \n",
            "B. `jsonify(amount=str(decimal_obj))`  \n",
            "C. `app.config['JSONIFY_PRETTYPRINT_REGULAR'] = False`  \n",
            "D. Pass `decimal_obj` directly to `jsonify()` and register a custom decoder.  \n",
            "\n",
            "Correct Answer: A  \n",
            "Explanation: A custom encoder keeps the numeric type in the wire format (`{amount: 3.50}`) and is reusable across the whole app. Converting to string (B) pollutes the schema, the pretty-print flag (C) is unrelated, and decoders (D) are for inbound data, not serialization.\n",
            "\n",
            "---\n",
            "\n",
            "**Question 3**  \n",
            "You need a thread-safe, per-worker cache that survives green-thread switching in Gunicorn gevent workers. Which solution fits best?\n",
            "\n",
            "A. `from gevent.lock import Semaphore; cache = {} with Semaphore()`  \n",
            "B. `import threading; threading.local()`  \n",
            "C. `from multiprocessing import Manager; Manager().dict()`  \n",
            "D. `from functools import lru_cache`  \n",
            "\n",
            "Correct Answer: A  \n",
            "Explanation: A dict protected by a gevent-native semaphore is coroutine-safe and survives implicit yielding; no pickling overhead. `threading.local()` (B) ties data to OS threads, which gevent reuses across greenlets; `Manager().dict()` (C) is cross-process but overkill inside one worker; `lru_cache` (D) is neither coroutine-safe nor worker-local.\n",
            "\n",
            "---\n",
            "\n",
            "**Question 4**  \n",
            "A data pipeline pickles `datetime.datetime` objects and sporadically fails unpickling on servers set to UTC after DST transitions. Which practice prevents this reliably?\n",
            "\n",
            "A. Store `dt.astimezone(datetime.timezone.utc)` before pickling.  \n",
            "B. Pickle `dt.isoformat()` instead of the object.  \n",
            "C. Use `pickle.HIGHEST_PROTOCOL`.  \n",
            "D. Set env-var `TZ=UTC` before unpickling.  \n",
            "\n",
            "Correct Answer: A  \n",
            "Explanation: Normalizing to aware UTC removes dependency on the server’s local zone and DST ambiguity, so unpickling never shifts wall-clock times. ISO strings (B) work but require extra parse overhead and schema changes; higher protocol (C) doesn’t address timezone issues; setting `TZ` (D) is brittle and affects the whole runtime.\n",
            "\n",
            "---\n",
            "\n",
            "**Question 5**  \n",
            "Your asyncio service must call a CPU-bound NumPy routine without blocking the event loop. Which pattern is correct?\n",
            "\n",
            "A. `loop.run_in_executor(None, heavy_numpy, *args)`  \n",
            "B. `await heavy_numpy(*args)`  \n",
            "C. `asyncio.create_task(heavy_numpy(*args))`  \n",
            "D. `asyncio.gather(*[heavy_numpy(arg) for arg in data])`  \n",
            "\n",
            "Correct Answer: A  \n",
            "Explanation: `run_in_executor` offloads the GIL-holding NumPy call to a thread-pool (or process-pool) so the loop stays responsive. Direct await (B) or `create_task` (C) still run in the main thread; `gather` (D) only parallelizes coroutines, not CPU work.\n",
            "\n",
            "---\n",
            "\n",
            "**Question 6**  \n",
            "You profile a PySpark job and see most time in Python `udf(df['ts'])` that converts Unix seconds to ISO strings. What is the best drop-in replacement?\n",
            "\n",
            "A. `df.withColumn('iso', F.from_unixtime('ts').cast('string'))`  \n",
            "B. Register the UDF in Scala and call via `df.select(scala_udf(df.ts))`  \n",
            "C. Vectorized UDF using `@pandas_udf(\"string\")`  \n",
            "D. Cache the dataframe before applying the UDF.  \n",
            "\n",
            "Correct Answer: A  \n",
            "Explanation: `from_unixtime` is a built-in JVM expression avoiding Python serialization overhead entirely. A Scala UDF (B) helps but adds JVM/JAR complexity; `pandas_udf` (C) is faster than row-wise UDF but still pays Arrow conversion; caching (D) doesn’t speed up the transform itself.\n",
            "\n",
            "---\n",
            "\n",
            "**Question 7**  \n",
            "A FastAPI endpoint uses `pydantic.BaseModel` with a UUID path parameter. You want OpenAPI to show the UUID format correctly and get automatic validation. How?\n",
            "\n",
            "A.  \n",
            "```python\n",
            "class Item(BaseModel):\n",
            "    id: UUID\n",
            "```  \n",
            "and declare path param with type `UUID`.  \n",
            "\n",
            "B.  \n",
            "```python\n",
            "class Item(BaseModel):\n",
            "    id: str\n",
            "```  \n",
            "and validate with regex inside `@validator('id')`.  \n",
            "\n",
            "C.  \n",
            "```python\n",
            "class Config:\n",
            "    json_encoders = {UUID: str}\n",
            "```  \n",
            "alone.  \n",
            "\n",
            "D. Use `uuid.UUID(item.id.hex)` inside the endpoint.  \n",
            "\n",
            "Correct Answer: A  \n",
            "Explanation: Declaring the parameter and field with Python’s `UUID` type leverages FastAPI’s automatic converter + validator, generating correct OpenAPI docs. Regex validator (B) is manual boilerplate; `json_encoders` (C) only affects serialization, not inbound validation; manual conversion (D) is too late and undocumented.\n",
            "\n",
            "---\n",
            "\n",
            "**Question 8**  \n",
            "You need exactly-once semantics when a Kafka-Python consumer processes messages and writes rows to PostgreSQL. Which approach is both idempotent and transactional?\n",
            "\n",
            "A. Consume → produce to a compacted topic keyed by business-id → use Kafka Streams.  \n",
            "B. Set `enable.idempotence=true`, `isolation.level=read_committed`, and wrap consume + insert in a Kafka transaction.  \n",
            "C. Store last processed offset in Redis and `INSERT ... ON CONFLICT DO NOTHING`.  \n",
            "D. Commit offsets manually after every insert.  \n",
            "\n",
            "Correct Answer: B  \n",
            "Explanation: Kafka’s EOS (exactly-once semantics) uses idempotent producers and transactional consumption so offset commit and output records are atomic. Redis (C) adds another system and still leaves a window; manual commit (D) is at-least-once if the insert succeeds but commit fails; option A merely moves data inside Kafka without solving the sink write.\n",
            "\n",
            "---\n",
            "\n",
            "**Question 9**  \n",
            "In Python 3.10 you want to parallelize I/O-bound coroutines while capping concurrency at 50. Which construct is most concise and correct?\n",
            "\n",
            "A. `asyncio.Semaphore(50)` acquired around each coroutine.  \n",
            "B. `asyncio.BoundedSemaphore(50)` inside an `asyncio.gather`.  \n",
            "C. `asyncio.Queue(maxsize=50)` with N worker coroutines.  \n",
            "D. `concurrent.futures.ThreadPoolExecutor(max_workers=50)`  \n",
            "\n",
            "Correct Answer: A  \n",
            "Explanation: A semaphore around `await` limits simultaneous coroutines without extra queue boilerplate. `BoundedSemaphore` (B) is fine but doesn’t differ here; `Queue` + workers (C) is more code for the same goal; `ThreadPoolExecutor` (D) targets threads, not asyncio coroutines.\n",
            "\n",
            "---\n",
            "\n",
            "**Question 10**  \n",
            "You are building a library that must stay compatible with Python 3.7–3.11 and wants to ship C extensions for speed. Which build frontend is officially recommended and future-proof?\n",
            "\n",
            "A. `python -m build` with `pyproject.toml` specifying `build-backend = \"setuptools.build_meta\"`.  \n",
            "B. `setup.py bdist_wheel` invoked directly.  \n",
            "C. `poetry build`  \n",
            "D. `pip wheel .`  \n",
            "\n",
            "Correct Answer: A  \n",
            "Explanation: PEP 517/518 `python -m build` is the PyPA blessed, isolated frontend; it works with any PEP 517 backend and produces wheels across versions. Direct `setup.py` (B) is deprecated; Poetry (C) locks you into its toolchain; `pip wheel` (D) is a client-side side-effect, not a library’s build interface.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "================================================================================\n",
            "GENERATING 10 MCQs → Python Automation (4-7 years) – DevOps / Test Automation\n",
            "================================================================================\n",
            "Which strategy best prevents `requests` sessions from ballooning memory in a long-running automation worker that polls an internal REST endpoint every 5 s?\n",
            "\n",
            "A. Create a new `requests.Session` per poll and rely on CPython’s GC to close it  \n",
            "B. Re-use a single global `Session` and call `session.cookies.clear()` every 100 iterations  \n",
            "C. Re-use a single global `Session` and add `Connection: close` headers to every request  \n",
            "D. Instantiate a new `Session` every hour and copy cookies from the old one with `update()`  \n",
            "\n",
            "Correct Answer: C  \n",
            "Explanation: A persistent `Session` keeps TCP connections and cookies alive; adding `Connection: close` lets the underlying urllib3 pool recycle sockets without accumulating stale SSL contexts or cookie jars. Clearing cookies (B) helps a bit but still leaks connection objects, while hourly copying (D) merely postpones the bloat and still multiplies file descriptors.\n",
            "\n",
            "---\n",
            "\n",
            "A nightly Jenkins job runs 2000 pytest-based API tests in parallel (`-n auto`) and starts failing with `OSError: [Errno 24] Too many open files`. Which fix is both correct and least invasive?\n",
            "\n",
            "A. Raise the worker-user `ulimit -n` on every agent to 65 k and keep the test code unchanged  \n",
            "B. Replace every `with open(...)` inside tests with `pathlib.Path(...).read_text()`  \n",
            "C. Add the `--dist=loadgroup` option so each worker gets fewer tests, reducing FD pressure  \n",
            "D. Refactor fixtures that return file handles to return file contents and close handles eagerly  \n",
            "\n",
            "Correct Answer: D  \n",
            "Explanation: The root cause is fixtures that keep files open across tests; eagerly reading contents and closing handles drops FD usage by orders of magnitude without touching infra. Bumping ulimits (A) masks the leak, and `loadgroup` (C) only redistributes the same leak across more processes.\n",
            "\n",
            "---\n",
            "\n",
            "You need to ship a self-contained Python CLI tool to 500 production hosts that run different OS versions and may lack internet. Packaging approach?\n",
            "\n",
            "A. Create a `requirements.txt` with pinned versions and run `pip install -r` on each host  \n",
            "B. Build a `manylinux` wheel for every dependency and copy wheels plus `venv` bootstrap script  \n",
            "C. Compile a single PE/ELF binary with `pyoxidizer`/`nuitka` and distribute it via Ansible  \n",
            "D. Copy the raw project directory and invoke `python3 -m pip install -e .` on each host  \n",
            "\n",
            "Correct Answer: C  \n",
            "Explanation: A compiled single artefact has no runtime dependency on system Python or pip and eliminates “works on my machine” pip resolution issues. Bundling wheels (B) still needs a compatible `venv` and pip on the host, while editable installs (D) assume build tools and internet.\n",
            "\n",
            "---\n",
            "\n",
            "An Ansible playbook that uses the `docker_container` module works from laptops but fails in CI with “Docker API 400 Bad Request – port already in use”. The CI runner is Docker-in-Docker. Best fix?\n",
            "\n",
            "A. Add `ports: \"127.0.0.1:{{ item }}:{{ item }}\"` to bind explicitly to the inner container’s loopback  \n",
            "B. Switch to `network_mode: host` so DinD re-uses the host’s network stack  \n",
            "C. Randomise host ports with `published_port: 0` and query them later via `docker_port` facts  \n",
            "D. Pre-define a unique port range per CI job via `environment:` and template it into the task  \n",
            "\n",
            "Correct Answer: C  \n",
            "Explanation: Letting Docker pick free ephemeral ports avoids collision entirely and keeps the playbook host-agnostic; querying the assigned port afterwards keeps tests reproducible. Binding to loopback inside DinD (A) still collides because the “inner” localhost is the same for all sibling containers. Host networking (B) breaks on rootless Docker.\n",
            "\n",
            "---\n",
            "\n",
            "A GitLab pipeline stage spins up a Kubernetes Job that runs `python -m pytest` against a micro-service. Tests randomly abort with “Connection reset by peer” to Postgres. Which action is most robust?\n",
            "\n",
            "A. Add `time.sleep(5)` before the first DB access to let the service stabilise  \n",
            "B. Wrap DB connection attempts in `tenacity.retry(stop=stop_after_attempt(10))` with exponential back-off  \n",
            "C. Increase the Job’s CPU request from 100 m to 500 m so pytest starts faster  \n",
            "D. Switch the Job’s `restartPolicy` from `Never` to `OnFailure` so Kubernetes retries the whole suite  \n",
            "\n",
            "Correct Answer: B  \n",
            "Explanation: Transient network blips during Pod startup are normal; client-side back-off with jitter tolerates them without hard-coding arbitrary sleeps or wasting CPU. Bumping CPU (C) does not fix race conditions, and `OnFailure` (D) reruns the entire test suite, hiding flaky tests instead of stabilising them.\n",
            "\n",
            "---\n",
            "\n",
            "Your `tox` matrix must validate code against Python 3.8, 3.9, 3.10 and 3.11 but GitHub runners label them as `3.8`, `3.9`, `3.10`, `3.11`. How do you reference the interpreter inside `tox.ini` so the same file works locally and in CI?\n",
            "\n",
            "A. Use `basepython = python3` and let tox pick the highest available  \n",
            "B. List `py38,py39,py310,py311` environments and set `actions/setup-python` with matching versions  \n",
            "C. Keep `envlist = py{38,39,310,311}` and set `basepython = python3.{38,39,310,311}`  \n",
            "D. Declare `skip_missing_interpreters = True` and rely on the runner to supply only one version  \n",
            "\n",
            "Correct Answer: B  \n",
            "Explanation: Explicitly aligning `envlist` with the exact patchless interpreters that `setup-python` makes available guarantees the matrix expands correctly both locally (where `py -3.10` may exist) and in CI. Dynamic `basepython` (C) fails because `python3.310` is not a valid binary name; skipping missing (D) silently collapses the matrix.\n",
            "\n",
            "---\n",
            "\n",
            "A legacy Fabric 1 script uses `run(\"pkill -f gunicorn\")` and fails when the process does not exist. Which Fabric 2/3 compatible replacement is cleanest?\n",
            "\n",
            "A. `c.run(\"pkill -f gunicorn\", warn=True, hide=True)`  \n",
            "B. `c.run(\"pkill -f gunicorn || true\")`  \n",
            "C. Use `c.run(\"systemctl stop gunicorn\")` instead and mandate systemd  \n",
            "D. Wrap in a try/except for `invoke.exceptions.UnexpectedExit` and pass  \n",
            "\n",
            "Correct Answer: A  \n",
            "Explanation: `warn=True` tells Invoke to return a result object with `.exited` set instead of raising, letting the caller decide without shell-specific tricks. The shell `|| true` (B) works but couples you to `/bin/sh` syntax; try/except (D) is verbose and still aborts the connection on other real failures.\n",
            "\n",
            "---\n",
            "\n",
            "A PyTorch model-training cronjob inside EKS randomly OOMKills after three hours. The container has memory limits but no requests. Best first step?\n",
            "\n",
            "A. Set memory request = limit so the scheduler reserves the full amount and evicts noisy neighbours  \n",
            "B. Enable vertical-pod-autoscaler in recommendation mode and apply its suggested request after 24 h  \n",
            "C. Add `PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128` to reduce fragmentation  \n",
            "D. Migrate the job to a node-group with 2× larger instances  \n",
            "\n",
            "Correct Answer: A  \n",
            "Explanation: Without a request, the kubelet may place the Pod on a node that cannot actually provide the limit, triggering OOMKill when neighbours grow; requesting the full amount guarantees the memory is held and prevents eviction. VPA (B) is reactive, and fragmentation tuning (C) helps only after memory is guaranteed.\n",
            "\n",
            "---\n",
            "\n",
            "You must write a pytest fixture that yields a running PostgreSQL instance for integration tests. Local engineers use Docker Desktop, CI uses Podman, and some run ARM Macs. Most portable approach?\n",
            "\n",
            "A. Build the image locally with `platform: linux/amd64` hard-coded and push to the team registry  \n",
            "B. Use the official `postgres` image and rely on Docker/Podman’s multi-arch manifest to pull the right variant  \n",
            "C. Spin up an ephemeral cloud RDS instance per test run and tear it down afterwards  \n",
            "D. Run PostgreSQL directly on the host via `brew install postgres` and expose port 5432  \n",
            "\n",
            "Correct Answer: B  \n",
            "Explanation: Official images on Docker Hub have multi-arch manifests; Docker Desktop and Podman both resolve to the correct architecture automatically, eliminating custom builds. Hard-coding AMD64 (A) forces emulation on ARM Macs, while RDS (C) introduces cost and latency, and host installs (D) pollute engineer machines.\n",
            "\n",
            "---\n",
            "\n",
            "A Slack bot built with `slack-bolt` and AWS Lambda (Python 3.9) cold-starts in 8 s, exceeding Slack’s 3 s timeout. Which change gives the biggest consistent speed-up?\n",
            "\n",
            "A. Package the zip with `zipapp` instead of plain zip to reduce uncompressed size  \n",
            "B. Provision 3 000 MB of memory so Lambda allocates 2 vCPUs and reduces init time  \n",
            "C. Add `lazy_listener=True` to all bolt listeners to defer heavy imports  \n",
            "D. Move the heavy `torch` import out of the handler and into a Lambda layer  \n",
            "\n",
            "Correct Answer: B  \n",
            "Explanation: Lambda allocates CPU proportional to memory; jumping from 128 MB to 3 000 MB typically cuts cold-start by 70 % because decompression and import happen in parallel on two cores. `lazy_listener` (C) helps only after the runtime is already initialised, and Lambda layers (D) still import on cold start without extra CPU.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "================================================================================\n",
            "GENERATING 10 MCQs → Deep Learning (PyTorch/TensorFlow) (3-5 years) – Computer Vision / NLP\n",
            "================================================================================\n",
            "You receive a 224×224 RGB image and need to fine-tune a torchvision ResNet-50 backbone for a 10-class classification task.  \n",
            "Which preprocessing pipeline is both GPU-accelerated and keeps the native ImageNet statistics intact?\n",
            "\n",
            "A. transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ConvertImageDtype(torch.float32), transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)])  \n",
            "B. transforms.Compose([transforms.Resize(256, antialias=True), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)])  \n",
            "C. transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])])  \n",
            "D. transforms.Compose([transforms.Resize(256, interpolation=InterpolationMode.BILINEAR), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)])  \n",
            "\n",
            "Correct Answer: B  \n",
            "Explanation: Option B is the only one that keeps the native ImageNet statistics and uses the antialias=True flag, giving a GPU-accelerated, differentiable resize with correct normalization. Option A is acceptable but omits antialias, leading to aliasing artifacts on modern GPUs, while C and D either change the statistics or miss the antialias optimization.\n",
            "\n",
            "---\n",
            "\n",
            "A BERT-base model is fine-tuned for token-level classification with TensorFlow 2.x. After 3 epochs the training loss plateaus while the validation loss rises. Which single action most efficiently combats this overfitting without degrading downstream accuracy?\n",
            "\n",
            "A. Add a tf.keras.layers.Dropout(0.5) right after the pooled-output layer  \n",
            "B. Reduce the learning-rate from 3e-5 to 1e-5 and reload the best checkpoint  \n",
            "C. Freeze the bottom 6 transformer layers and continue training only the top 6  \n",
            "D. Replace AdamW with SGD+momentum and keep the same learning-rate  \n",
            "\n",
            "Correct Answer: C  \n",
            "Explanation: Freezing the lower layers regularizes the model by reducing trainable parameters while preserving the high-level semantics learned in the upper layers, a technique proven effective for token-level tasks. Option B is plausible but merely slows the optimizer without changing capacity, whereas A and D either hurt expressiveness or destabilize pre-trained weights.\n",
            "\n",
            "---\n",
            "\n",
            "During mixed-precision training of a U-Net in PyTorch 1.13 you observe occasional NaNs. Which minimal code change reliably prevents them without hurting throughput?\n",
            "\n",
            "A. Wrap the optimizer with torch.cuda.amp.GradScaler(enabled=False)  \n",
            "B. Add torch.autograd.set_detect_anomaly(True) at the top of the script  \n",
            "C. Insert torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) after scaler.step(optimizer)  \n",
            "D. Replace ReLU activations with LeakyReLU(negative_slope=0.01)  \n",
            "\n",
            "Correct Answer: C  \n",
            "Explanation: Gradient clipping inside the autocast context stops rare but extreme gradient spikes that cause NaNs under fp16 dynamic range, while keeping the speed benefit of mixed precision. Option D is acceptable but changes the model capacity; A disables scaling entirely, and B is only a debugging tool.\n",
            "\n",
            "---\n",
            "\n",
            "You export a Vision Transformer (ViT-B/16) to TorchScript for production. The forward pass runs but the traced graph fails on dynamic batch sizes. Which tracing flag fixes the issue with the smallest runtime cost?\n",
            "\n",
            "A. torch.jit.trace(model, example_inputs=randn(1,3,224,224), strict=False)  \n",
            "B. torch.jit.script(model)  \n",
            "C. torch.jit.trace(model, example_inputs=(randn(1,3,224,224),), check_trace=False)  \n",
            "D. torch.jit.trace(model, example_inputs=randn(8,3,224,224), strict=True)  \n",
            "\n",
            "Correct Answer: A  \n",
            "Explanation: strict=False allows tracing to record control-flow-free graphs that tolerate variable batch dimensions, whereas full scripting (B) is overkill and slower. Option C hides shape mismatch but does not solve the batching problem, and D hard-codes an arbitrary batch size.\n",
            "\n",
            "---\n",
            "\n",
            "In TF 2.11 you need to shard a 1.2-TFRecord ImageNet dataset across 8 GPUs with tf.data. Which pipeline guarantees true global shuffling every epoch without dropping samples?\n",
            "\n",
            "A. files.shuffle(1024).repeat().shard(8, worker_index).interleave(tf.data.TFRecordDataset, cycle_length=8).shuffle(2048).batch(batch_size)  \n",
            "B. files.shard(8, worker_index).shuffle(1024).interleave(tf.data.TFRecordDataset).shuffle(10000).batch(batch_size)  \n",
            "C. files.shuffle(2048).interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.AUTOTUNE).shuffle(50000).shard(8, worker_index).batch(batch_size)  \n",
            "D. files.interleave(tf.data.TFRecordDataset, cycle_length=8).shard(8, worker_index).shuffle(2048).batch(batch_size)  \n",
            "\n",
            "Correct Answer: C  \n",
            "Explanation: Sharding after the second global shuffle ensures each worker sees a different random subset every epoch without duplication or omission. Option A shards too early, leading to data repetition, while B and D perform only local shuffles and miss global randomness.\n",
            "\n",
            "---\n",
            "\n",
            "You convert a PyTorch ConvNeXt backbone to TensorRT 8.5 via ONNX. The exported engine has 3× more latency than the FP32 PyTorch model. Which ONNX export argument is the most likely fix?\n",
            "\n",
            "A. opset_version=17  \n",
            "B. do_constant_folding=False  \n",
            "C. export_params=False  \n",
            "D. input_names=[“images”], output_names=[“features”], dynamic_axes=None  \n",
            "\n",
            "Correct Answer: A  \n",
            "Explanation: ConvNeXt relies on GroupNorm and other ops that require opset ≥ 14 for TensorRT fusion; opset 17 unlocks the fastest plugin implementations. Option B is acceptable but only affects graph size, whereas C and D are wrong because they either drop weights or disable dynamic shapes unnecessarily.\n",
            "\n",
            "---\n",
            "\n",
            "When pruning channels from a ResNet-18 trained with PyTorch 2.0, which structured-pruning strategy preserves the pre-trained BatchNorm statistics without requiring a full re-training loop?\n",
            "\n",
            "A. Use torch.nn.utils.prune.ln_structured(amount=0.3, n=2, dim=0) on conv layers and let BatchNorm layers inherit the mask  \n",
            "B. Iteratively rank channels by L1 magnitude, zero corresponding BN weight, then apply BN.eval() before fine-tuning 1 epoch  \n",
            "C. Apply magnitude pruning to conv.weight, recompute BN.running_mean/var on a single calibration mini-batch, then fine-tune with LR 1e-4  \n",
            "D. Replace each conv+BN block with a fused ConvBn2d and prune inside the fused module  \n",
            "\n",
            "Correct Answer: C  \n",
            "Explanation: One forward pass with BN in eval mode recalibrates the running statistics to the new channel set, allowing 1-epoch fine-tuning to recover accuracy. Option A is acceptable but unstructured, while B zeros weights without removing channels and D changes the model definition.\n",
            "\n",
            "---\n",
            "\n",
            "A production pipeline uses TensorFlow Serving with a EfficientDet-D0 object-detection model. Client-side JPEG decoding becomes the throughput bottleneck. Which TF-Data option removes this bottleneck with the least code change?\n",
            "\n",
            "A. Set tf.data.Options.experimental_optimization.map_parallelization = True  \n",
            "B. Use tf.image.decode_jpeg inside tf.data.Dataset.map with num_parallel_calls=tf.data.AUTOTUNE  \n",
            "C. Switch the serving signature to accept raw uint8 tensors and decode on GPU with tf.image.decode_image  \n",
            "D. Convert the dataset to TFRecord containing pre-decoded PNG and drop JPEG entirely  \n",
            "\n",
            "Correct Answer: C  \n",
            "Explanation: Moving decode_image to the GPU inside the model lets TF-Serving overlap JPEG decoding with GPU compute, eliminating the CPU-bound client step. Option B is plausible but still keeps decoding on CPU, whereas A and D require dataset rewrites or format changes.\n",
            "\n",
            "---\n",
            "\n",
            "You train a RoBERTa-large model with DeepSpeed ZeRO-3 on 4 A100-80GB GPUs. Which configuration change reduces the optimizer-state memory footprint to 1/3 without increasing wall-clock time?\n",
            "\n",
            "A. Set “optimizer”: {“type”: “AdamW”, “params”: {“betas”: [0.9, 0.999], “eps”: 1e-6}} and “zero_optimization”: {“stage”: 3, “offload_optimizer”: {“device”: “cpu”}}  \n",
            "B. Replace AdamW with fused AdaFactor and keep ZeRO-3 enabled  \n",
            "C. Enable gradient checkpointing via model.gradient_checkpointing_enable()  \n",
            "D. Increase batch size so that gradient accumulation steps drop from 8 to 2  \n",
            "\n",
            "Correct Answer: B  \n",
            "Explanation: AdaFactor maintains competitive convergence without storing the second momentum, cutting optimizer-state memory by ≈2/3 under ZeRO-3 while fused kernels keep speed. Option A offloads to CPU and slows training; C trades compute for memory and D does not reduce optimizer states.\n",
            "\n",
            "---\n",
            "\n",
            "A PyTorch DataLoader with num_workers=8 and pin_memory=True still shows CPU-side starvation when training Swin-Transformer on 8 V100s. Which single additional argument removes the stall?\n",
            "\n",
            "A. Replace default collate_fn with torch.utils.data.dataloader.default_collate  \n",
            "B. Set persistent_workers=True  \n",
            "C. Increase prefetch_factor from 2 to 8  \n",
            "D. Set multiprocessing_context=“spawn”  \n",
            "\n",
            "Correct Answer: C  \n",
            "Explanation: Raising prefetch_factor lets each worker preload more batches ahead, hiding the CPU augmentation latency from the GPU stream. Option B is acceptable but only saves worker spawn cost, whereas A and D address unrelated issues.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "# Put your Groq API key here (get it free at https://console.groq.com)\n",
        "client = OpenAI(\n",
        "    api_key=\"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\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\"\n",
        ")\n",
        "\n",
        "def generate_skill_mcqs(\n",
        "    skill: str,\n",
        "    years: str = \"3-5 years\",\n",
        "    industry: str = None,\n",
        "    num_questions: int = 10\n",
        "):\n",
        "    system_prompt = \"\"\"You are an expert technical interviewer with 15+ years of industry experience.\n",
        "You create extremely high-quality multiple-choice questions that perfectly discriminate candidates with real-world experience.\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"Generate exactly {num_questions} multiple-choice questions for the following profile:\n",
        "\n",
        "Skill                : {skill}\n",
        "Required Experience  : {years}\n",
        "Industry/Context     : {industry or \"Any\"}\n",
        "\n",
        "RULES (follow exactly):\n",
        "- Each question must have exactly 4 options (A, B, C, D)\n",
        "- Exactly ONE option is the best/correct answer\n",
        "- Exactly ONE option is plausible/acceptable but not the best (common mistake, outdated practice, or junior-level thinking)\n",
        "- The other two options must be clearly wrong but believable distractors\n",
        "- After the options, write on separate lines:\n",
        "  Correct Answer: [LETTER]\n",
        "  Explanation: [2-4 sentence detailed explanation why this is correct and why the acceptable one is not the best]\n",
        "- Questions must require real {years} experience to answer confidently\n",
        "- Never repeat questions\n",
        "- Cover different sub-topics of the skill\n",
        "\n",
        "Output only the questions (no intro, no numbering at the beginning).\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"moonshotai/kimi-k2-instruct-0905\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=4096,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    skills = [\n",
        "        (\"Python\", \"3-5 years\", \"General Backend / Data Engineering\"),\n",
        "        (\"Python Automation\", \"4-7 years\", \"DevOps / Test Automation\"),\n",
        "        (\"Deep Learning (PyTorch/TensorFlow)\", \"3-5 years\", \"Computer Vision / NLP\"),\n",
        "    ]\n",
        "\n",
        "    for skill, years, industry in skills:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"GENERATING 10 MCQs → {skill} ({years}) – {industry}\")\n",
        "        print('='*80)\n",
        "        mcqs = generate_skill_mcqs(skill, years, industry)\n",
        "        print(mcqs)\n",
        "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "        # Optional: save to file\n",
        "        safe_name = skill.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
        "        with open(f\"mcqs_{safe_name}_{years}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"Skill: {skill}\\nExperience: {years}\\nIndustry: {industry}\\n\\n\")\n",
        "            f.write(mcqs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improve Prompt Style"
      ],
      "metadata": {
        "id": "X83EF3WSPjWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# Get your free key from https://console.groq.com\n",
        "client = OpenAI(\n",
        "    api_key=\"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\",          # replace with your real key\n",
        "    base_url=\"https://api.groq.com/openai/v1\"\n",
        ")\n",
        "\n",
        "def generate_mcqs_with_labels(\n",
        "    skill: str,\n",
        "    years: str = \"3-5 years\",\n",
        "    industry: str = None,\n",
        "    num_questions: int = 10\n",
        "):\n",
        "    system_prompt = \"\"\"You are an expert technical interviewer with 15+ years of real-world experience.\n",
        "You create extremely high-quality MCQs that perfectly separate junior, mid-level and senior candidates.\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"Generate exactly {num_questions} multiple-choice questions for this profile:\n",
        "\n",
        "            Skill               : {skill}\n",
        "            Experience required : {years}\n",
        "            Industry/Context    : {industry or \"General\"}\n",
        "\n",
        "            STRICT OUTPUT FORMAT (never deviate):\n",
        "            Q1. <question text>\n",
        "\n",
        "            A) <option A>\n",
        "            B) <option B>\n",
        "            C) <option C>\n",
        "            D) <option D>\n",
        "\n",
        "            Correct Answer       : <LETTER>\n",
        "            Acceptable Answer    : <LETTER> (plausible but not the best — common mistake, outdated practice or junior-level solution)\n",
        "            Distractor           : <LETTER>\n",
        "            Distractor           : <LETTER>\n",
        "\n",
        "            Explanation: <2–4 sentences explaining why Correct is best, why Acceptable is not ideal, and why the two distractors are wrong>\n",
        "\n",
        "            <blank line between questions>\n",
        "\n",
        "            Rules:\n",
        "            - Exactly 4 options (A–D)\n",
        "            - Exactly 1 Correct, exactly 1 Acceptable, exactly 2 Distractors\n",
        "            - Questions must need real {years} experience to answer confidently\n",
        "            - Cover different sub-topics\n",
        "            - Output ONLY the questions (no intro text, no markdown fences, no extra numbering)\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"moonshotai/kimi-k2-instruct-0905\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\",   \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0.72,      # slight creativity but very consistent format\n",
        "        max_tokens=8192,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# ──────── EXAMPLE USAGE ────────\n",
        "if __name__ == \"__main__\":\n",
        "    topics = [\n",
        "        (\"Python\", \"3-5 years\", \"Backend / Data Engineering\"),\n",
        "        (\"Python Automation\", \"4-7 years\", \"DevOps / QA Automation\"),\n",
        "        (\"Deep Learning with PyTorch/TensorFlow\", \"3-6 years\", \"Computer Vision or NLP\"),\n",
        "    ]\n",
        "\n",
        "    for skill, years, industry in topics:\n",
        "        print(f\"\\n{'='*100}\")\n",
        "        print(f\"10 MCQs → {skill} | {years} | {industry or 'General'}\")\n",
        "        print('='*100)\n",
        "        result = generate_mcqs_with_labels(skill, years, industry)\n",
        "        print(result)\n",
        "\n",
        "        # Save to file\n",
        "        filename = f\"MCQs_{skill.replace(' ', '_').replace('/', '-')}_{years}.txt\"\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"Skill: {skill} | Experience: {years} | Context: {industry}\\n\\n\")\n",
        "            f.write(result)\n",
        "        print(f\"Saved to {filename}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT8RLL6rN2G4",
        "outputId": "f041399f-e87e-4933-cd4c-b90420559a4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "10 MCQs → Python | 3-5 years | Backend / Data Engineering\n",
            "====================================================================================================\n",
            "Q1. A nightly batch job loads 50 GB of JSON Lines from S3 into an Amazon Aurora PostgreSQL cluster using `psycopg2`.  \n",
            "Which combination of techniques will give the best end-to-end throughput while keeping CPU below 40 % on the writer instance?\n",
            "\n",
            "A) Use `COPY ... FROM STDIN` with `psycopg2.extras.copy_from()` on a single connection and compress the files with gzip  \n",
            "B) Split the file into 8 MB chunks, spawn 12 worker processes that each open a fresh connection and call `COPY ... FROM STDIN` with `psycopg2.extras.copy_from()`  \n",
            "C) Read the file in 8 MB chunks, wrap them in `io.StringIO` and use `executemany()` with `ON CONFLICT DO NOTHING`  \n",
            "D) Convert the JSON Lines to CSV, upload to S3, then run `aws s3 cp` into an Aurora table using the Aurora S3 integration\n",
            "\n",
            "Correct Answer       : B  \n",
            "Acceptable Answer    : A  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: `COPY` is the fastest bulk-load path in Postgres; parallelising across many small chunks keeps the writer busy without saturating any single backend. Single-connection `COPY` (A) is safe but becomes CPU-bound on the one backend process. `executemany()` (C) issues row-by-row inserts and is orders of magnitude slower. The Aurora S3 import (D) is server-side but still single-threaded and adds unnecessary CSV conversion.\n",
            "\n",
            "Q2. You need to expose a Pandas DataFrame (8 M rows, 30 columns) to a FastAPI endpoint as compressed Parquet.  \n",
            "Which snippet streams the response without ever materialising the full file in RAM?\n",
            "\n",
            "A) `df.to_parquet(\"/tmp/tmp.parq\"); return FileResponse(\"/tmp/tmp.parq\", media_type=\"application/octet-stream\")`  \n",
            "B) `buf = io.BytesIO(); df.to_parquet(buf); buf.seek(0); return StreamingResponse(buf, media_type=\"application/octet-stream\")`  \n",
            "C) `buf = io.BytesIO(); df.to_parquet(buf); return Response(buf.getvalue(), media_type=\"application/octet-stream\")`  \n",
            "D) `stream = io.BytesIO(); pq.write_table(pa.Table.from_pandas(df), stream); return StreamingResponse(stream, media_type=\"application/octet-stream\")`\n",
            "\n",
            "Correct Answer       : D  \n",
            "Acceptable Answer    : B  \n",
            "Distractor           : A  \n",
            "Distractor           : C  \n",
            "\n",
            "Explanation: `pyarrow.parquet.write_table` accepts any file-like object and writes incrementally, so the `BytesIO` object can be yielded chunk-by-chunk by `StreamingResponse`. Option B also uses streaming but first converts the whole DataFrame in memory, which still spikes RAM. Option A materialises a temp file and blocks on disk I/O. Option C loads the entire buffer into the response object, defeating streaming.\n",
            "\n",
            "Q3. A Spark job that ran fine on 100 GB suddenly fails with “Container killed by YARN for exceeding memory limits” after the data grew to 400 GB.  \n",
            "Which change is most likely to fix the problem without increasing executor memory?\n",
            "\n",
            "A) Increase `spark.sql.shuffle.partitions` from 200 to 1200  \n",
            "B) Replace `groupBy(...).count()` with `reduceByKey(lambda a,b: a+b)`  \n",
            "C) Set `spark.sql.adaptive.enabled=false` to avoid skewed partitions  \n",
            "D) Cache the largest intermediate DataFrame with `df.cache()` before the join\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : B  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: More shuffle partitions shrink individual partition size, keeping heaps under the limit; this is the canonical fix for data growth. `reduceByKey` (B) helps only when you can pre-aggregate, not for plain counts. Disabling AQE (C) removes skew mitigation and worsens the problem. Caching (D) pins the full DataFrame in memory, increasing pressure.\n",
            "\n",
            "Q4. You maintain a Python service that must guarantee idempotent inserts into BigQuery for duplicate Pub/Sub deliveries.  \n",
            "The table has no primary key, and you want minimal query cost.  \n",
            "Which design meets the requirement?\n",
            "\n",
            "A) Add a `message_id` column and run `MERGE` using that column as the update key  \n",
            "B) Let the inserts fail silently with `insertErrors` and deduplicate nightly with a `CREATE OR REPLACE` dedup query  \n",
            "C) Hash the entire row and use the hash as `insertId`; BigQuery ignores duplicates within 1 min  \n",
            "D) Create a unique `message_id` column in the table and rely on BigQuery to reject duplicates\n",
            "\n",
            "Correct Answer       : C  \n",
            "Acceptable Answer    : A  \n",
            "Distractor           : B  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: BigQuery uses `insertId` for exactly-once ingestion within a one-minute dedup window; hashing the row gives a stable id. `MERGE` (A) works but incurs query costs for every duplicate. Option B is eventual-only and risks exposing duplicates to downstream consumers. BigQuery does not enforce uniqueness constraints (D), so inserts would succeed and create duplicates.\n",
            "\n",
            "Q5. A Flask API runs under Gunicorn with gevent workers. After adding a new endpoint that uses `requests` to call a REST service, response latency spikes and worker warnings appear: “MaxRequestsExceeded”.  \n",
            "Which single-line change solves the issue without touching Gunicorn config?\n",
            "\n",
            "A) Replace `requests.get(url)` with `gevent.monkey.patch_all()` at import time  \n",
            "B) Replace `requests.get(url)` with `httpx.AsyncClient` and `async/await`  \n",
            "C) Replace `requests.get(url)` with `requests.get(url, timeout=2)`  \n",
            "D) Replace `requests.get(url)` with `urllib3.PoolManager().request(\"GET\", url)`\n",
            "\n",
            "Correct Answer       : B  \n",
            "Acceptable Answer    : A  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: `requests` is blocking; under gevent you must use an async library (`httpx` with `AsyncClient`) so the worker can yield the socket. Monkey-patching (A) has already been done by Gunicorn, so it changes nothing. A timeout (C) only prevents hung workers, not concurrency. `urllib3` (D) is still blocking.\n",
            "\n",
            "Q6. A Celery task uploads large parquet files to GCS. Tasks randomly fail with “BrokenPipeError” when the worker is under load.  \n",
            "Which worker setting best removes the flakiness?\n",
            "\n",
            "A) Set `worker_prefetch_multiplier = 1`  \n",
            "B) Set `task_acks_late = True`  \n",
            "C) Set `worker_max_tasks_per_child = 1000`  \n",
            "D) Set `worker_pool = 'solo'`\n",
            "\n",
            "Correct Answer       : D  \n",
            "Acceptable Answer    : A  \n",
            "Distractor           : B  \n",
            "Distractor           : C  \n",
            "\n",
            "Explanation: The default `prefork` pool shares open file descriptors among forked processes; under load the GCS upload socket can be closed by another process. Running `solo` (no fork) avoids this. Lowering prefetch (A) helps but does not fix the FD inheritance issue. Late ack (B) and max tasks (C) are unrelated to socket lifetime.\n",
            "\n",
            "Q7. A data pipeline uses SQLAlchemy Core to insert 5 M rows into Postgres.  \n",
            "Which pattern gives the highest throughput while keeping rollback segments reasonable?\n",
            "\n",
            "A) `connection.execute(MyTable.__table__.insert(), rowdicts)` with `autocommit=True`  \n",
            "B) `connection.execute(MyTable.__table__.insert().values(rowdicts))`  \n",
            "C) `connection.execute(MyTable.__table__.insert(), rowdicts.chunksize(10000))` inside one transaction  \n",
            "D) `copy_command = \"COPY my_table FROM STDIN WITH (FORMAT CSV)\"; cursor.copy_expert(copy_command, file=csvfile)`\n",
            "\n",
            "Correct Answer       : D  \n",
            "Acceptable Answer    : C  \n",
            "Distractor           : A  \n",
            "Distractor           : B  \n",
            "\n",
            "Explanation: Postgres `COPY` is the fastest bulk path and keeps WAL and rollback segments small because it bypasses much of the MVCC overhead. Chunked Core inserts (C) are safe but still row-by-row from the server’s perspective. Single huge `values()` (B) creates a multi-megabyte statement and can bloat rollback. Autocommit (A) commits every row, crushing throughput.\n",
            "\n",
            "Q8. A PySpark job reads 2 TB of ORC files and filters on `event_date >= '2023-10-01'`.  \n",
            "The filter is pushed down but still scans 1.8 TB.  \n",
            "Which action will prune the scan to the few hundred GB that actually match?\n",
            "\n",
            "A) Repartition by `event_date` and overwrite the ORC files  \n",
            "B) Convert to Parquet and create a partitioned external table by `event_date`  \n",
            "C) Add an ORC bloom filter on `event_date`  \n",
            "D) Cache the DataFrame before the filter\n",
            "\n",
            "Correct Answer       : B  \n",
            "Acceptable Answer    : A  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: Partitioning the directory layout (`event_date=YYYY-MM-DD/`) lets Spark read only matching directories; Parquet is preferred because ORC does not expose partition pruning to Spark in the same way. Repartitioning ORC (A) helps but still requires listing all files. Bloom filters (C) work only for point lookups, not range. Caching (D) does not reduce initial read.\n",
            "\n",
            "Q9. A Python service must serialise datetimes to ISO-8601 strings with microsecond precision and timezone ‘Z’ for downstream Java systems.  \n",
            "Which snippet always produces the correct format regardless of system locale?\n",
            "\n",
            "A) `dt.isoformat(timespec='microseconds')`  \n",
            "B) `dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')`  \n",
            "C) `dt.astimezone(timezone.utc).isoformat(timespec='microseconds').replace('+00:00', 'Z')`  \n",
            "D) `f\"{dt:%Y-%m-%dT%H:%M:%S}.{dt.microsecond // 1000:03d}Z\"`\n",
            "\n",
            "Correct Answer       : C  \n",
            "Acceptable Answer    : B  \n",
            "Distractor           : A  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: `isoformat` preserves microseconds only if the datetime is timezone-aware in UTC; replacing ‘+00:00’ with ‘Z’ yields valid RFC 3339. `strftime` (B) works but needs the `%f` directive which is locale-safe yet longer to type. Option A omits timezone conversion so naive datetimes will not append ‘Z’. Option D truncates microseconds to milliseconds and is hard-coded.\n",
            "\n",
            "Q10. A FastAPI service uses `uvicorn` with `--workers 4`. One endpoint CPU-intensively resamples a 500 MB Pandas DataFrame with `df.resample('1min').mean()`.  \n",
            "Under load the endpoint becomes 10× slower than in development.  \n",
            "Which fix restores speed without changing the algorithm?\n",
            "\n",
            "A) Set the env var `OMP_NUM_THREADS=1` before starting uvicorn  \n",
            "B) Switch uvicorn worker class to `uvloop`  \n",
            "C) Replace CPython with PyPy for the workers  \n",
            "D) Add `df.to_numpy()` before resampling to speed up the calculation\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : C  \n",
            "Distractor           : B  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: By default each worker spawns as many BLAS threads as cores; with 4 workers the machine oversubscribes and thrashes. Limiting OpenMP to 1 thread per worker lets uvicorn’s multiprocessing scale. PyPy (C) helps but is a bigger change. `uvloop` (B) improves asyncio, not CPU-bound Pandas. Converting to numpy (D) is redundant since `resample` already works on numpy arrays under the hood.\n",
            "Saved to MCQs_Python_3-5 years.txt\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "10 MCQs → Python Automation | 4-7 years | DevOps / QA Automation\n",
            "====================================================================================================\n",
            "Q1. Your GitLab pipeline runs a nightly pytest suite that intermittently fails when the test container starts before the PostgreSQL service is healthy. The pipeline spins up both containers via docker-compose. Which strategy best eliminates the race condition without adding arbitrary sleep calls?\n",
            "\n",
            "A) Use docker-compose v2.4+ healthcheck with service_healthy condition in depends_on  \n",
            "B) Wrap pytest in a 30-second sleep before the first connection attempt  \n",
            "C) Add --retry 5 --retry-delay 10 to the pip install command that installs psycopg2-binary  \n",
            "D) Replace docker-compose with docker run --restart=on-failure for each container\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : B  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: Health-aware depends_on guarantees the test container starts only after Postgres reports healthy, eliminating the flake without brittle sleeps. A fixed sleep sometimes works but wastes time and can still fail under load. Retry on pip or container restart does not address the root ordering problem.\n",
            "\n",
            "Q2. A legacy Jenkins job stores test reports as 1.2 GB of scattered XML files in workspace/.  You need to keep them for 30 days, make them searchable by test-name and build-number, and avoid inflating Jenkins’ disk. Which Python-centric approach is most robust?\n",
            "\n",
            "A) Compress each XML with gzip, upload to S3 using boto3 with standard storage class and a 30-day lifecycle rule  \n",
            "B) rsync the folder to an NFS mount and run a nightly find -mtime +30 -delete via cron  \n",
            "C) tar the folder and attach it to the build as an artifact; let Jenkins’ artifact retention policy delete after 30 days  \n",
            "D) Parse every XML into dicts, insert into a local SQLite DB and drop the files\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : C  \n",
            "Distractor           : B  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: S3 with lifecycle is limitless, cheap, and keeps Jenkins lean; compression halves transfer/storage. Relying on Jenkins artifact storage bloats the master and can crash it when hundreds of builds accumulate. rsync+NFS lacks built-in expiry and couples you to fragile infra; SQLite ingestion is overkill and loses original files.\n",
            "\n",
            "Q3. You are automating release notes generation from 500+ commits across 12 repos since the last tag. Conventional commits are enforced, but Jira ticket numbers live only in the body, not the title. Which Python tool-chain yields the cleanest markdown notes grouped by fix/feat/BREAKING?\n",
            "\n",
            "A) git-town + commitizen + jinja2 template that regex-extracts Jira IDs from commit.body  \n",
            "B) Parse git log with --format=%B into pandas, groupby ticket, then hand-roll markdown  \n",
            "C) Run github-changelog-generator in a container and post-process with sed  \n",
            "D) Use semantic-release with the default configuration and no plugins\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : B  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: commitizen understands conventional commits and the jinja2 template can flexibly inject ticket links; git-town keeps history linear. Pandas approach works but is heavy and reinvents wheels. github-changelog-generator ignores body tickets and needs Ruby; vanilla semantic-release won’t pick Jira IDs at all.\n",
            "\n",
            "Q4. While parallelising a 90-minute pytest suite with pytest-xdist you observe that the session crashes with “Internal error: could not serialize” when workers return custom objects. What is the safest fix that keeps the parallel speed-up?\n",
            "\n",
            "A) Replace return of custom objects with lightweight dicts or namedtuples and keep tests stateless  \n",
            "B) Switch to pytest-parallel which uses threads instead of processes  \n",
            "C) Set --dist=loadfile so each file runs in one worker  \n",
            "D) Downgrade to pytest 4.x which predates the serialisation check\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : C  \n",
            "Distractor           : B  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: xdist serialises results via pickle; custom classes without __reduce__ break. Converting to plain data keeps the suite stateless and fast. loadfile avoids the issue but may imbalance load. Threading still serialises returns and risks GIL contention; downgrading loses years of bug-fixes.\n",
            "\n",
            "Q5. Your team wants to gate releases on 99 %-tile API latency < 200 ms. You have k6 running in the CI pipeline but results are noisy because the shared GitLab runners are over-provisioned. Which Python-based adjustment gives the most stable pass/fail signal?\n",
            "\n",
            "A) Run k6, parse its end-of-test summary JSON with Python, pull CPU steal from /proc/stat, normalise latency with linear regression, then assert  \n",
            "B) Switch the job to a dedicated bare-metal runner tagged \"perf\" and assert k6’s http_req_duration p99  \n",
            "C) Wrap k6 with a Python retry loop that re-runs the test up to 5 times until p99 < 200 ms  \n",
            "D) Replace k6 with locust and measure the greenlet’s avg response time\n",
            "\n",
            "Correct Answer       : B  \n",
            "Acceptable Answer    : A  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: Contention is best solved by removing it; a tagged runner gives reproducible hardware and no steal time. Regression normalisation is clever but still noisy and over-engineered. Retry until pass hides regressions and wastes minutes. Locust greenlet average is not p99 and co-routines introduce jitter.\n",
            "\n",
            "Q6. You maintain a Python CLI that provisions AWS dev environments. Users report that boto3 occasionally throws “ExpiredToken” when the job runs longer than 1 h. Credentials come from an IAM role via OIDC from GitHub Actions. Which single-line change permanently fixes the issue?\n",
            "\n",
            "A) Upgrade botocore and set AWS_STS_REGIONAL_ENDPOINTS=regional; boto3 will auto-refresh  \n",
            "B) Add botocore.credentials.RefreshableCredentials.with_autorefesh() manually  \n",
            "C) Re-export AWS_* keys every 55 minutes using a background cron job  \n",
            "D) Set AWS_ROLE_SESSION_NAME to a static value instead of the default random UUID\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : B  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: Modern botocore refreshes role credentials automatically; the env-var forces regional STS which avoids global endpoint caches. Manual refresh is obsolete boiler-plate. Re-exporting is racy and brittle; session name uniqueness is irrelevant to expiry.\n",
            "\n",
            "Q7. A pytest fixture spins up a local MinIO container for S3 integration tests. After moving to Apple M1 laptops the container fails with “exec format error”. Which approach keeps the same Python test code and works on both x86_64 CI and ARM dev laptops?\n",
            "\n",
            "A) Use MinIO’s multi-arch manifest and pull minio/minio:latest without platform flag; let Docker choose  \n",
            "B) Force platform=linux/amd64 and rely on QEMU emulation  \n",
            "C) Build a custom MinIO image on the laptop with buildx and push to the local registry  \n",
            "D) Replace MinIO with moto server running in Python, no Docker needed\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : D  \n",
            "Distractor           : B  \n",
            "Distractor           : C  \n",
            "\n",
            "Explanation: MinIO’s official image supports both architectures under the same tag, so Docker will pull the native variant; no change in Python code. Emulation is slow and flaky; building custom images is busywork. Moto is acceptable but changes the integration surface from real S3-API to a mock.\n",
            "\n",
            "Q8. You need to enforce that every new pull request contains at least one update to the CHANGELOG.md in keep-a-changelog format. Which Python-based GitHub App strategy gives the earliest feedback without blocking the ability to merge when the PR is still a draft?\n",
            "\n",
            "A) Probot with a Python micro-service that listens to pull_request synchronize, skips draft=true, and sets a failing status check  \n",
            "B) A GitHub Action that runs only on pull_request_target, checks for changelog diff, and uses the GITHUB_TOKEN to comment  \n",
            "C) Pre-commit hook in the repo with a Python script that greps for changelog entries; enforce via branch protection  \n",
            "D) Weekly batch job that scans merged PRs and opens retroactive style-fix PRs\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : B  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: A GitHub App can post a status check instantly, skip drafts, and is centrally managed. pull_request_target Actions work but are repo-scoped and need careful token scoping. Pre-commit is client-side and can be bypassed; a weekly batch job is far too late.\n",
            "\n",
            "Q9. Your Python automation package is installed via pip in a RHEL-8 production venv that already ships Python 3.6. You want to use match statements (3.10+) without breaking the system Python. Which path requires the least operational change and keeps the venv self-contained?\n",
            "\n",
            "A) Build a portable Python 3.11 with pyenv, compile wheels with manylinux, deliver them via pip --find-links and activate the venv against 3.11  \n",
            "B) Package the tool in a manylinux wheel that bundles the 3.11 interpreter using zipapp  \n",
            "C) Rewrite the match into if/elif ladders and stay on 3.6  \n",
            "D) Install python3.11 system RPM from EPEL and symlink /usr/bin/python3\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : B  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: pyenv lets you compile a specific Python without touching system packages; the venv points to the new interpreter and wheels stay standard. A bundled interpreter inside a wheel is possible but huge and non-standard. Rewriting loses modern syntax; symlinking system Python risks breaking yum.\n",
            "\n",
            "Q10. A long-running Airflow DAG uses a PythonOperator that shells out to aws s3 sync. After Airflow 2.4 upgrade tasks randomly fail with “Killed due to memory”. dmesg shows oom-killer. Which code-level fix best preserves the sync semantics while staying within the 2 Gi container limit?\n",
            "\n",
            "A) Replace subprocess.run([\"aws\", \"s3\", \"sync\"...]) with boto3’s S3Transfer and set multipart_chunksize to 50 MB  \n",
            "B) Add swap space in the Dockerfile so OOM is avoided  \n",
            "C) Set Airflow task retries=5 and hope it passes on rerun  \n",
            "D) Move the sync into a KubernetesPodOperator with 4 Gi request\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : D  \n",
            "Distractor           : B  \n",
            "Distractor           : C  \n",
            "\n",
            "Explanation: boto3 transfer streams chunks and uses ~50 MB RAM instead of loading keys into the AWS CLI Python process plus forked child. Adding swap only masks memory pressure and makes the container thrash. Retries without fixing the footprint are futile; a bigger pod works but avoids optimising the root cause.\n",
            "Saved to MCQs_Python_Automation_4-7 years.txt\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "10 MCQs → Deep Learning with PyTorch/TensorFlow | 3-6 years | Computer Vision or NLP\n",
            "====================================================================================================\n",
            "Q1. You are training a U-Net for medical-image segmentation on 512×512 volumes. After switching from PyTorch’s `CrossEntropyLoss` to `DiceLoss` the validation Dice coefficient improves but the borders of the predicted mask become noisy. Which change will most effectively reduce the border noise while keeping the gain in overall Dice?\n",
            "\n",
            "A) Replace `DiceLoss` with `DiceBCELoss` (weighted sum of Dice and BCE) and tune the weight λ.  \n",
            "B) Add a boundary-aware term that penalizes the L2 distance between predicted and ground-truth distance-transform maps.  \n",
            "C) Increase the weight of the foreground class in `CrossEntropyLoss` and keep the original U-Net.  \n",
            "D) Freeze encoder weights and fine-tune only the decoder with `DiceLoss` and heavy data augmentation.\n",
            "\n",
            "Correct Answer       : B  \n",
            "Acceptable Answer    : A  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: A boundary-aware term (B) directly regularizes the border pixels without giving up the Dice gain. A is acceptable because the BCE component improves border sharpness but still treats every pixel independently, so it is less principled than an explicit boundary loss. C returns to the original loss that already under-performed, and D does nothing to address the loss function’s border behavior.\n",
            "\n",
            "Q2. Your TensorFlow 2.x training pipeline for a vision transformer (ViT) is I/O bound: GPU utilization is ~30 %. The dataset contains 10 M 224×224 JPEGs stored on a single NVMe SSD. Which single change gives the largest throughput boost on the current hardware?\n",
            "\n",
            "A) Convert images to TFRecord with `tf.image.encode_jpeg(quality=95)` and turn on `tf.data.AUTOTUNE`.  \n",
            "B) Add `prefetch(tf.data.AUTOTUNE)` right after `map()` and keep raw JPEG decoding on CPU.  \n",
            "C) Convert images to WebP inside TFRecord and set `num_parallel_calls=1` to reduce seek overhead.  \n",
            "D) Move the `decode_jpeg` op into `tf.data.map()` with `tf.device('/gpu:0')`.\n",
            "\n",
            "Correct Answer       : B  \n",
            "Acceptable Answer    : A  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: Prefetching (B) overlaps data loading with GPU computation and is the quickest win when the bottleneck is already the CPU preprocessing. A helps but JPEG decoding stays on CPU and 95 % quality keeps files large. WebP (C) shrinks file size but `num_parallel_calls=1` serializes decoding and hurts throughput. Decoding on GPU (D) is unsupported for JPEG in TF 2.x and will throw an error.\n",
            "\n",
            "Q3. In PyTorch you register a forward hook to collect the feature map of `layer4` in a ResNet-50. During mixed-precision training with `autocast` the saved tensor is `float16`. You need `float32` for offline PCA without blowing up memory. Which approach keeps the tensor as `float32` while minimally impacting speed?\n",
            "\n",
            "A) Inside the hook cast the tensor to `float32` with `.float()` before appending to a list.  \n",
            "B) Disable `autocast` for the whole epoch.  \n",
            "C) Register the hook under `torch.cuda.amp.autocast(enabled=False)` context.  \n",
            "D) Save the tensor in `float16` and run PCA in mini-batches on GPU.\n",
            "\n",
            "Correct Answer       : C  \n",
            "Acceptable Answer    : A  \n",
            "Distractor           : B  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: Using `autocast(enabled=False)` (C) around the hook keeps the rest of the network in mixed precision while forcing only the hook output to `float32`, avoiding extra copies. A works but adds a device-side cast for every forward. B defeats the purpose of AMP and slows training. D stays in `float16` and accumulates unacceptable numerical error for PCA.\n",
            "\n",
            "Q4. You fine-tune a Hugging Face BERT model for token-level classification with `Trainer` and `AdamW`. After adding a new token to the tokenizer and resizing embeddings, loss stays flat. Which check is most likely to reveal the bug?\n",
            "\n",
            "A) Verify that `model.resize_token_embeddings()` also resized the output classifier head.  \n",
            "B) Ensure `tokenizer.add_special_tokens()` was called before `resize_token_embeddings()`.  \n",
            "C) Check that the learning-rate scheduler was restarted after resizing.  \n",
            "D) Confirm that `data_collator` ignores `-100` labels.\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : B  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: `resize_token_embeddings()` only grows the embedding matrix; the randomly initialized weights for new tokens must be propagated to any tied output layer (A). Missing this step keeps the output head untrained and loss flat. B is plausible but the order is not the root cause. C and D are unrelated to the embedding resize issue.\n",
            "\n",
            "Q5. A PyTorch object-detection model uses `FocalLoss` with default γ=2. You observe that foreground classes are still heavily mis-classified. Which γ adjustment together with a balanced change to α best improves mAP without grid-searching both hyper-parameters independently?\n",
            "\n",
            "A) Increase γ to 3 and keep α=0.25.  \n",
            "B) Decrease γ to 1 and set α equal to the inverse class frequency.  \n",
            "C) Increase γ to 5 and set α=0.75.  \n",
            "D) Keep γ=2 and set α=0.05.\n",
            "\n",
            "Correct Answer       : B  \n",
            "Acceptable Answer    : A  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: Lower γ (B) reduces the down-weighting of easy negatives, while setting α to inverse frequency directly tackles class imbalance; together they improve foreground recall. Higher γ (A,C) suppresses negatives even more, aggravating foreground under-representation. D keeps γ=2 but shrinks α, further starving the rare class.\n",
            "\n",
            "Q6. You export a TensorFlow SavedModel that uses `tf.nn.sigmoid_cross_entropy_with_logits` to TensorFlow-Lite for mobile deployment. The 8-bit quantized model’s accuracy drops 5 %. Which post-training quantization strategy recovers the most accuracy with the smallest latency increase?\n",
            "\n",
            "A) Full-integer quantization with representative dataset and `optimizations=[tf.lite.Optimize.DEFAULT]`.  \n",
            "B) Integer-only quantization of weights but keep activations in `float16`.  \n",
            "C) Integer quantization with `tf.lite.OpsSet.SELECT_TF_OPS` fallback.  \n",
            "D) Quantization-aware training (QAT) starting from the floating checkpoint.\n",
            "\n",
            "Correct Answer       : D  \n",
            "Acceptable Answer    : B  \n",
            "Distractor           : A  \n",
            "Distractor           : C  \n",
            "\n",
            "Explanation: QAT (D) simulates quantization during training and gives near-fp32 accuracy with full 8-bit inference. `float16` activations (B) recover some accuracy but still run quantized weights and add partial fp16 compute. Plain full-integer (A) without QAT cannot recover the 5 % drop, and SELECT_TF_OPS (C) increases binary size and latency.\n",
            "\n",
            "Q7. In a PyTorch GAN for text-to-image synthesis you notice generator loss drifting to zero while discriminator loss saturates. Image diversity collapses. Which modification best stabilizes training under the Wasserstein regime?\n",
            "\n",
            "A) Clip discriminator weights to ±0.01 and train with `lr=1e-5`.  \n",
            "B) Switch to WGAN-GP with λ=10 gradient penalty and keep `lr=1e-4`.  \n",
            "C) Add spectral normalization to the generator and use hinge loss.  \n",
            "D) Use history-based replay buffer for generated images and keep original GAN loss.\n",
            "\n",
            "Correct Answer       : B  \n",
            "Acceptable Answer    : A  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: WGAN-GP (B) enforces Lipschitz constraint via gradient penalty, curing saturation without weight clipping side-effects. A is the original WGAN trick but clipping slows convergence and hurts sample quality. Spectral norm on generator (C) does not address discriminator saturation. Replay buffer (D) helps mode collapse but not loss saturation.\n",
            "\n",
            "Q8. You implement a custom TensorFlow layer that uses dynamic 2-D convolutions whose filters depend on the input. During graph export with `tf.function` you get “ValueError: Tried to convert a tensor of shape (None, …) to a TensorArray”. Which fix preserves dynamic shapes and keeps XLA compatibility?\n",
            "\n",
            "A) Build filters inside `def call()` using `tf.nn.convolution` and mark the layer `@tf.function(experimental_relax_shapes=True)`.  \n",
            "B) Use `tf.TensorArray` with `size=0, dynamic_size=True` inside `call()`.  \n",
            "C) Replace dynamic conv with grouped convolutions and pre-compute filters.  \n",
            "D) Move filter creation to `def build()` and store as `self.add_weight`.\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : B  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: Relaxing shapes (A) lets `tf.function` trace unknown spatial dimensions while keeping filters input-dependent. `TensorArray` (B) works but disables XLA. Grouped conv (C) changes the algorithm and cannot express input-conditioned filters. Storing filters as weights (D) forces static shapes and fails when the filter count varies per sample.\n",
            "\n",
            "Q9. A PyTorch multi-GPU setup uses `DistributedDataParallel` with `find_unused_parameters=False`. You add an auxiliary segmentation head that is only active for half of the batches. Training hangs at the first `loss.backward()`. Which single change eliminates the hang with minimal overhead?\n",
            "\n",
            "A) Set `find_unused_parameters=True` in `DistributedDataParallel`.  \n",
            "B) Call `torch.cuda.synchronize()` before every backward.  \n",
            "C) Use `torch.autograd.set_detect_anomaly(True)`.  \n",
            "D) Zero the auxiliary head loss for batches that do not use it and keep `find_unused_parameters=False`.\n",
            "\n",
            "Correct Answer       : A  \n",
            "Acceptable Answer    : D  \n",
            "Distractor           : B  \n",
            "Distractor           : C  \n",
            "\n",
            "Explanation: Unused parameters produce `None` gradients that NCCL all-reduce cannot handle; enabling `find_unused_parameters=True` (A) lets the reducer skip them. Zeroing the loss (D) still leaves unused parameters, so the hang persists. `synchronize()` (B) and anomaly mode (C) do not affect gradient collection and add no remedy.\n",
            "\n",
            "Q10. You deploy a TensorRT 8 engine from a PyTorch transformer encoder for real-time NLP inference on NVIDIA Jetson. The engine was built with `kHALF` and `kSTRICT_TYPES` and `max_workspace_size=1 GB`. At runtime the first inference call takes 50 ms while subsequent calls take 4 ms. Which change most effectively reduces the first-call latency without increasing the engine size?\n",
            "\n",
            "A) Build the engine with `kDIRECT_IO` and `kPREFER_PRECISION_CONSTRAINTS`.  \n",
            "B) Run an empty `context.execute_async` once after engine deserialization.  \n",
            "C) Serialize the engine with `kSTRIP_PLAN` flag and reload.  \n",
            "D) Switch to `kFLOAT` instead of `kHALF`.\n",
            "\n",
            "Correct Answer       : B  \n",
            "Acceptable Answer    : A  \n",
            "Distractor           : C  \n",
            "Distractor           : D  \n",
            "\n",
            "Explanation: TensorRT allocates GPU memory and performs auto-tuning on the first inference; a warm-up run (B) hides this cost from the latency-sensitive path. `kDIRECT_IO` (A) helps only if the plugin demands it and does not skip the setup. `kSTRIP_PLAN` (C) merely reduces disk size, and `kFLOAT` (D) doubles the model size and hurts throughput.\n",
            "Saved to MCQs_Deep_Learning_with_PyTorch-TensorFlow_3-6 years.txt\n",
            "\n"
          ]
        }
      ]
    }
  ]
}